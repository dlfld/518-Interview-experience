## **一、机器学习**

### **贝叶斯分类和极大似然估计**

**贝叶斯分类**：用先验概率和类条件概率求后验概率                     ![img](https://docimg8.docs.qq.com/image/l36PtVv1eb4KvQfzv5zpAA.png?w=253&h=79)        

**极大似然估计**：一种参数估计方法。用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！

换句话说，极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”

**重要前提**：训练样本的分布能代表样本的真实分布。每个样本集中的样本都是所谓独立同分布的随机变量 (iid条件)，且有充分的训练样本。

**一般步骤**:求最大似然估计量：

（1）写出似然函数；

（2）对似然函数取对数，并整理；

（3）求导数；

（4）解似然方程。

**特点**：

  1.比其他估计方法更加简单；

  2.收敛性：无偏或者渐近无偏，当样本数目增加时，收敛性质会更好；

  3.如果假设的类条件概率模型正确，则通常能获得较好的结果。但如果假设模型出现偏差，将导致非常差的估计结果。

**朴素贝叶斯为何朴素**：因为它假设在实际数据中几乎不可能看到：条件概率被计算为组件个体概率的纯乘积。 这意味着特征的绝对独立性 – 这种情况在现实生活中可能永远不会遇到。

### **概率和似然的区别**

概率和似然都是指可能性。概率描述了已知参数时的随机变量的输出结果；似然则用来描述已知随机变量输出结果时，未知参数的可能取值。

### **偏差和方差**

偏差:是衡量预测值与真实值的关系,是指预测值与真实值之间的差值.

方差:是衡量预测值之间的关系,和真实值无关.也就是他们的离散程度

误差 = 偏差 + 方差

目标就是追求低偏差并且低方差

### **ROC和AUC**

Roc (Receiver operating characteristic) 曲线是一种二元分类模型分类效果的分析工具。伪阳性率(FPR)定义为 X 轴，真阳性率(TPR)定义为 Y 轴。

真阳性率TPR: 在所有实际为阳性的样本中，被正确地判断为阳性之比率 TPR = TP/P = TP/(TP+FN)

伪阳性率FPR: 在所有实际为阴性的样本中，被错误地判定为阳性之比率 FPR = FP/N = FP/(FP + TN)

给定一个二元分类模型和它的阈值，就能从所有样本的（阳性／阴性）真实值和预测值计算出一个 (X=FPR, Y=TPR) 座标点。

AUC 最普遍的定义是 ROC 曲线下的面积。但其实另一种定义更常用，分别随机从正负样本集中抽取一个正样本，一个负样本，正样本的预测值大于负样本的概率。

AUC优势：AUC的计算方法同时考虑了分类器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器作出合理的评价。



### **L1 L2正则**

L1正则化就是在loss function后边所加正则项为L1范数，加上L1范数容易得到稀疏解（0比较多）。L2正则化就是loss function后边所加正则项为L2范数的平方，加上L2正则相比于L1正则来说，得到的解比较平滑（不是稀疏），但是同样能够保证解中接近于0（但不是等于0，所以相对平滑）的维度比较多，降低模型的复杂度



### **如何解决过拟合？**

1. 获取和使用更多的数据集，或交叉验证
2. 加入正则
3. 采用合适的模型，过于复杂的模型会带来过拟合问题。
4. Dropout。Dropout 指的是在训练过程中每次按一定的几率关闭或忽略某些层的节点。使得模型在使用同样的数据进行训练时相当于从不同的模型中随机选择一个进行训练
5. Early Stopping 是参数微调中的一种，即在每个循环结束一次以后（这里的循环可能是 full data batch,也可能是 mini batch size），计算模型的准确率（accuracy）。当准确率不再增加时就停止训练。
6. 可变化的学习率 adam
7. 使用 Batch_Normalization 真正进入激活函数之前需要对其进行一次归一化计算



### **如何解决数据不平衡**

**1从数据角度**

主动获取：获取更多的少量样本数据

算法采样：上采样、下采样、生成合成数据

ADASYN SMOTE

改变权重

数据增强

**2从算法角度**

选择对数据倾斜相对不敏感的算法。如树模型等。

采用集成学习

迁移学习

课程学习

### **LightGBM相对于XGBoost的优化****!!!**





## **二、计算机基础**

### **数组和链表 优缺点**

数组：在内存中，数组是一块连续的区域。需要预留空间，在使用前要先申请占内存的大小，可能会浪费内存空间。插入数据和删除数据效率低。随机读取效率很高。不利于扩展，数组定义的空间不够时要重新定义数组。

链表：在内存中可以存在任何地方，不要求连续。增加数据和删除数据很容易。查找数据时效率低。易于扩展。

### **进程线程协程**

进程：是并发执行的程序在执行过程中分配和管理资源的基本单位，是一个动态概念，竞争计算机系统资源的基本单位。

线程：是进程的一个执行单元，是进程内科调度实体。比进程更小的独立运行的基本单位。线程也被称为轻量级进程。

协程：是一种比线程更加轻量级的存在。一个线程也可以拥有多个协程。其执行过程更类似于子例程，或者说不带返回值的函数调用。





### **JAVA Python 区别优势**

区别：

本质区别：Python 是一种脚本语言，可直接通过脚本引擎运行。Java语言是编译和解释型语言，需要编译后运行。

Python 变量是动态的，JAVA变量是静态的，需要声明。

JAVA是面向对象的，python是面向过程+面向对象。

Java偏向于商业开发/团队合作，Python偏向于数据分析

Python由于脚本语言，和动态变量导致运行速度较慢，可通过使用C/C++重写的库来提高速度。Java运行速度快，效率高。

Python生态更全面，导包更快。

Python广播机制适合矩阵运算，在两个维度不同的矩阵运算时可以向维度为1的方向广播。例如一个m*n的矩阵a，让它加减乘除一个1*n的矩阵b，b它会被复制m次，成为一个m*n的矩阵，然后再逐元素地进行加减乘除操作。

### **C JAVA 垃圾回收**

**Java:**

四种垃圾回收算法：

引用计数:每次对象引用后加一个计数器，当一定时间无对象引用后列入垃圾回收队列。每次对象赋值都要维护计数器，效率低消耗大，无法处理循环引用。

复制：将内存划分为三个区域，Eden和survivorFrom\To。复制→清空→交换

标记清除：先标记可回收区域，之后清除，但是会产生内存碎片。

标记整理：标记清除基础上，再次扫描，向一端滑动对象，获取连续内存空间。缺点是承担移动对象的代价。

四种垃圾回收器：

串行GC（Serial）：单线程gc，会停止用户线程

并行GC（Parallel）：多线程共同gc，速度快，同样停止用户线程，适用科学计算/大数据处理，弱交互场景

并发GC（CMS）：支持GC与用户线程同时执行，互联网公司多用，响应时间有要求时使用。

**CMS收集器(Concurrent Mark Sweep:并发标记清除):目标最快回收停顿时间，适合网站和B/S系统的服务器。适合堆内存大，cpu核心多的服务器。

步骤：

1. 初始标记：标记GC root能关联的对象
2. 并发标记（和用户同时进行）：GCroot跟踪，主要标记过程，标记全部对象
3. 重新标记：正式清理前的再次确认
4. 并发清除（和用户同时进行）：清除GC root不可达对象。

G1：CMS升级版，不物理区分新生区和老年区，把内存分割为独立的子区域。有整理内存功能，不会产生内存碎片，支持预测停顿时间，用户指定期望停顿时间。多增加大型对象区域。

Java 8默认 并行GC	



C语言实际上是没有垃圾回收机制的，用保守垃圾收集器并调用free函数释放内存空间。

C++提供了基于引用计数算法的智能指针进行内存管理。



### **OOP（封装 继承 多态）**

创造出对象，包含若干属性和方法（函数）。

OOP的三大特征

•封装：把内部实现包裹起来，只需要提供使用方法即可

•继承：子对象可以继承父对象属性或方法，还可拓展自我属性和方法。可以增加代码的可重用性，拓展，修改。

•多态：能够重写继承父对象的方法，满足自身灵活需求。



## **三****NLP**

### **Bert**

基本情况：



缺点：

1. BERT在第一个预训练阶段，假设句子中多个单词被Mask掉，这些被Mask掉的单词之间没有任何关系，是条件独立的，然而有时候这些单词之间是有关系的。如 New York。(影响不大)
2. BERT的在预训练时会出现特殊的[MASK]，但是它在下游的fine-tune中不会出现，这就造成了预训练和微调之间的不匹配，微调不出现[MASK]这个标记，模型好像就没有了着力点、不知从哪入手。所以只将80%的替换为[mask]，但这也只是缓解、不能解决
3. 相较于传统语言模型，Bert的每批次训练数据中只有 15% 的标记被预测，这导致模型需要更多的训练步骤来收敛。
4. BERT的MASK方式为静态MASK，即15%的Tokens一旦选择就不再改变，也就是说一开始随机选择了15%的Tokens进行MASK处理，在接下来的N个epoch阶段中不再改变。
5. BERT是基于自编码的语言模型，在生成方面的NLP任务上表现效果不是很好
6. MASK时，因为BERT拆词的时候采用的是word piece 方法，可能会将一个单词划分成几个部分，MASK时会用[MASK]替换原属于一个单词中的一个部分，这种情况在预测[MASK]处的词，通过使用单词完整性就可以预测出来，用不到上下文信息。



优化：

1. 针对MASK时对单词的部分字符进行MASK，可以采用Whole Word Masking技术进行优化，即在MASK时，会MASK一个单词全部字符
2. RoBerta中对BERT的静态MASK方法进行改进，使用动态MASK进行MASK，即每次输入预训练语料前进行MASK，动态选取被MASK的Tokens。

### **RoBERTa**

改进：

1. 数据生成方式和任务改进：取消下一个句子预测，并且数据连续从一个文档中获得。
2. 更大更多样性的数据：使用 30G 中文训练，包含 3 亿个句子，100 亿个字 (即 token）。由于新闻、社区讨论、多个百科，保罗万象，覆盖数十万个主题，共160+G数据。
3. Byte-Pair Encoding（BPE）是字符级和词级别表征的混合。
4. 训练更久：超过500k steps
5. 更大批次：使用了超大（8k）的批次 batch size。
6. 调整Adam优化器的参数。二阶矩估计的超参数从0.999减小到0.98。防止分母为0的超参数从1e-8增大到1e-6。
7. 使用全词 mask（whole word mask）和动态的mask。

### **Attention**

Attention的种类：self-attention hard-attention、soft-attention、local-attention。

self-attention :                  ![img](https://docimg5.docs.qq.com/image/c3rKLQK8BIm2n8MMwXduJw.png?w=583&h=95)        

soft-attention又叫global-attention，考虑了所有的key并计算概率权重，最后加权求和。优点是考虑了所有的信息，缺点是计算量较大。

hard-attention直接精确定位到某个key，把这个key的概率设为1，其他key的概率设为0。优点是计算量小，缺点是对对齐方式要求高，没有对齐的话，影响较大。另外不可导，只能通过强化学习的方法进行训练。

local-attention是对一个窗口区域进行计算，综合soft-attention和hard-attention实现的，先使用hard-attention定位到某个点，以这个点为中心区域可以得到一个区域，最后在这个区域上使用soft计算soft-attention。



Self-attention:

优点:是不包含任何RNN、CNN结构，解决序列的长程依赖问题, 并行化矩阵计算非常快速。

缺点：

1. 由于没有任何部分处理序列信息，因此需要额外的positional encoding模块；
2. 注意矩阵维度等于输入序列长度的平方，输入序列较长的话非常消耗内存，目前的transformer最多能处理512长度的序列；
3. 对短程的上下文信息反而处理的不好

### **TF****-IDF**

L1正则化就是在loss function后边所加正则项为L1范数，加上L1范数容易得到稀疏解（0比较多）。L2正则化就是loss function后边所加正则项为L2范数的平方，加上L

### **Subword****（BPE）**

与空格分割分词相比的优势：

1、传统词表示方法无法很好的处理未知或罕见的词汇（OOV问题）

2、传统方法不利于模型学习词缀之间的关系

3、字符级别词嵌入粒度太细



Byte Pair Encoding，BPE双字节编码：

目的：BPE算法通过训练能把单词本意和时态分开，从而缩小词表。

流程：

1.准备足够大的训练语料

2.确定期望的subword词表大小

3.将单词拆分为字符序列并在末尾添加后缀“ </ w>”，统计单词频率。

4.统计每一个连续字节对的出现频率，选择最高频者合并成新的subword

5.重复第4步直到达到第2步设定的subword词表大小或下一个最高频的字节对出现频率为1

特点：词表会先变大再变小，最终完成压缩。

### **Batch****-****Norm和Layer****-****Norm**

归一化可以加速网络训练，防止梯度消失梯度爆炸。特征分布稳定时，可以让模型更好的学习特征的分布，更快收敛。

Batch-Norm一般用于CV领域，而Layer-Norm一般用于NLP领域。

Batch-Norm使用正态分布公式将一个Batch中相同维度的参数进行归一化。不同的实例的相同维度的相对关系被保留，而同一实例的不同维度特征则失去了可比性。

Layer-Norm 使用正态分布公式每个实例的参数向量进行归一化，保留了同一实例的不同维度特征可比性。

NLP不使用Batch-Norm原因：

(1)一个batch中不同句子字符长度不等，虽然通过补0或截断后能达到相同的句子长度，对这样一个batch进行BatchNorm，反而会加大特征的方差。

(2) NLP中一个batch中所有Tokens的第i维向量关联性不大，对他们进行BatchNorm会损失Tokens之间的差异性。而我们想保留不同Tokens之间的差异性，所以不在该维度进行Norm 

### **激活函数**

**Sigmoid**: 映射到0~1之间

公式：y = 1/ (1 + e^(-x))

导数：y (1 - y)

Tanh: 映射到-1~1之间

公式：y = (e^x - e^(-x))/(e^x + e^(-x))

导数：1 - y^2

Relu: y = max(0,x)

优点：

1、没有饱和区，不存在梯度消失问题，防止梯度弥散；

2、稀疏性；

3、没有复杂的指数运算，计算简单、效率提高；

4、实际收敛速度较快，比 Sigmoid/tanh 快很多；

5、比 Sigmoid 更符合生物学神经激活机制。

Gelu:                  ![img](https://docimg6.docs.qq.com/image/49nThRqzKAcgw004GiDBBA.png?w=787&h=63)        

Relu升级版，被证实了效果相当或更好。

​                 ![img](https://docimg3.docs.qq.com/image/z_EFFaiT4jMKPFJEd5A1Wg.png?w=850&h=563)        

### **X****xx**

xxx Batc



## **四、计网 计组 操作系统**

三次握手、tcpip

## **五****、数据结构**

各类排序算法





## **六、算法--动态规划**

